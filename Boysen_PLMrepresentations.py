# Pass sequences through ESM Transformer Model
# Joanne Boysen
# Luo Lab


# Imports
import torch
import glob
from torch import nn
from torch import Tensor
from torch.utils.data import DataLoader
import esm
import os
import random
import pandas as pd

BATCH_SIZE = 10

def load_inputs(folder):
    '''
    Load in input data from ProteinGym DMS substitution assays
    
    Parameters
    ----------
    folder: name of folder DMS assay files are in (str)

    Returns
    -------
    data: mutations and their respective sequences
        list of tuples
    '''
    
    seqs = []
    dms_scores = []
    mutations = []
    
    # for each DMS assay, load in mutations and sequences
    for file in glob.glob(folder + "/*.csv"):
        print(file)
        df = pd.read_csv(file, sep=',|:', engine='python')
        # select m number of random rows
        
        mutations.extend(df['mutant'])
        seqs.extend(df['mutated_sequence'])
        dms_scores.extend(df['DMS_score'])

    data = []
    # create list of tuples
    for i in range(len(mutations)):
        data.append((mutations[i], seqs[i]))

    return data, dms_scores, mutations

data, dms_scores, mutations = load_inputs('ProteinGym_substitutions/')
print('All ProteinGym data loaded')

def extract_mutation_representation(plm_reps, muts):
    '''
    Extract mutation representation from protein language model

    Parameters
    ----------
    plm_reps: batch of 3D tensor generated by ESM PLM model
    muts: batch of list of mutations 
        string

    Returns
    -------
    m_reps: 2D tensor with only representations for each mutation
    '''
    mut_nums = [int(m[1:-1]) for m in muts]

    m_reps = torch.empty(size=(BATCH_SIZE, token_representations.size()[2]))
    for i in range(BATCH_SIZE):
        m = mut_nums[i] - 1
        m_reps[i] = plm_reps[i,m,0:token_representations.size()[2]]

    return m_reps

with open('mutations_out.txt', 'w') as f:
    for item in mutations:
        f.write("%s\n" % item)
f.close()

with open('dms_out.txt', 'w') as f2:
    for item in dms_scores:
        f2.write("%s\n" % item)
f2.close()

# Load ESM model

model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()   # esm2_t33_650M_UR50D()   # 
batch_converter = alphabet.get_batch_converter()
model.eval()  # disables dropout for deterministic results

# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)
# replace with sequences from protein gym
#import pdb; pdb.set_trace()


for i in range(0, len(data), BATCH_SIZE):
    print(f'Batch index: {i}')
    data_batch = data[i:i+BATCH_SIZE]
    mutations_batch = mutations[i:i+BATCH_SIZE]
    num = 2500 + i
    
    batch_labels, batch_strs, batch_tokens = batch_converter(data_batch)
    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)

    # Extract per-residue representations (on CPU)
    with torch.no_grad():
        results = model(batch_tokens, repr_layers=[6], return_contacts=True)
        
    token_representations = results["representations"][6] # change to number of layers when changing models
    print(f'PLM representations tensor size: {token_representations.size()}')

    m_reps = extract_mutation_representation(token_representations, mutations_batch)
    print(f'Mutation representation tensor size: {m_reps.size()}')
    file_path = 'Pickles2/pickles_' + str(num) + '.t'
    torch.save(m_reps, file_path)






# import pdb; pdb.set_trace()
